"{\n  \"task_name\": \"train_models\",\n  \"module_responsibility\": \"This module is responsible for the training loop of the deep learning models. It takes the compiled model architectures (LSTM and Transformer) and the preprocessed, split datasets (training and validation) as input. Its sole focus is to execute the training process, monitor performance on the validation set, and save the best-performing model weights and training history. It does not handle data loading, preprocessing, model definition, or final test set evaluation.\",\n  \"model_role_and_purpose\": \"The purpose of this module is to use the training dataset to teach the LSTM and Transformer models the underlying patterns of stock price movements. By iteratively adjusting their internal weights based on a loss function, the models learn to predict the next day's closing price. The validation set is used concurrently to prevent overfitting, ensuring that the models generalize well to data they have not been trained on. This module produces the two core artifacts: the trained models.\",\n  \"concrete_tasks\": [\n    {\n      \"task_id\": \"train_models_1\",\n      \"name\": \"train_model\",\n      \"description\": \"A function to train a given model on the training data, using the validation data for performance monitoring and to save the best model.\",\n      \"inputs\": [\n        {\n          \"name\": \"model\",\n          \"type\": \"tensorflow.keras.Model or torch.nn.Module\",\n          \"description\": \"The compiled, un-trained model architecture (either LSTM or Transformer).\"\n        },\n        {\n          \"name\": \"X_train\",\n          \"type\": \"numpy.ndarray\",\n          \"format\": \"3D array of shape (num_samples, sequence_length, num_features) with float32 values.\"\n        },\n        {\n          \"name\": \"y_train\",\n          \"type\": \"numpy.ndarray\",\n          \"format\": \"1D array of shape (num_samples,) with float32 values.\"\n        },\n        {\n          \"name\": \"X_val\",\n          \"type\": \"numpy.ndarray\",\n          \"format\": \"3D array of shape (num_samples, sequence_length, num_features) with float32 values.\"\n        },\n        {\n          \"name\": \"y_val\",\n          \"type\": \"numpy.ndarray\",\n          \"format\": \"1D array of shape (num_samples,) with float32 values.\"\n        },\n        {\n          \"name\": \"training_params\",\n          \"type\": \"dict\",\n          \"description\": \"A dictionary containing hyperparameters for training, e.g., {'epochs': 50, 'batch_size': 32}.\"\n        }\n      ],\n      \"outputs\": [\n        {\n          \"name\": \"trained_model\",\n          \"type\": \"tensorflow.keras.Model or torch.nn.Module\",\n          \"description\": \"The model with the weights that achieved the best performance on the validation set.\"\n        },\n        {\n          \"name\": \"training_history\",\n          \"type\": \"dict\",\n          \"description\": \"A dictionary or history object containing lists of training and validation loss values for each epoch. Example: {'loss': [0.1, 0.08, ...], 'val_loss': [0.12, 0.09, ...]}\"\n        }\n      ]\n    }\n  ],\n  \"dependencies\": [\n    \"split_data\",\n    \"build_models\"\n  ],\n  \"constraints\": {\n    \"library_versions_and_configurations\": \"Use TensorFlow >= 2.10 or PyTorch >= 1.12. Training should be configured to use a GPU if available to accelerate the process. The Adam optimizer is recommended.\",\n    \"error_handling\": \"The module must handle potential training errors. `ValueError` should be raised for mismatched input data shapes. Framework-specific exceptions (e.g., TensorFlow's `tf.errors.ResourceExhaustedError` for out-of-memory issues) should be caught, logged with details, and can suggest reducing batch size or model complexity. The function should not suppress exceptions but let them propagate to the caller for system-level handling.\",\n    \"input_formats_and_data_types\": \"Inputs (`X_train`, `y_train`, `X_val`, `y_val`) must be NumPy arrays of type `numpy.float32`. The model input must be a compiled instance of `tf.keras.Model` or a `torch.nn.Module` with a defined forward pass.\",\n    \"output_formats_and_data_types\": \"The primary outputs are the trained model object and a history object/dictionary. The trained model should be saved to disk in the framework's standard format (e.g., .h5 for Keras). The history object's data (training/validation loss) must be accessible for the `visualize_results` module.\",\n    \"specific_error_handling\": \"Log start and end of training for each model. At each epoch, log the training and validation loss to a file or standard output for real-time monitoring. Implement EarlyStopping to halt training if validation loss does not improve for a set number of epochs (e.g., 10), preventing wasted computation and overfitting.\"\n  },\n  \"code_skeleton\": \"import tensorflow as tf\\nfrom typing import Tuple, Dict, Any\\nimport numpy as np\\n\\ndef train_model(model: tf.keras.Model, \\n                X_train: np.ndarray, \\n                y_train: np.ndarray, \\n                X_val: np.ndarray, \\n                y_val: np.ndarray, \\n                training_params: Dict[str, Any]) -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\\n    \\\"\\\"\\\"\\n    Trains a given Keras model on the provided training and validation datasets.\\n\\n    Args:\\n        model (tf.keras.Model): The compiled Keras model to be trained.\\n        X_train (np.ndarray): Training data features.\\n        y_train (np.ndarray): Training data targets.\\n        X_val (np.ndarray): Validation data features.\\n        y_val (np.ndarray): Validation data targets.\\n        training_params (Dict[str, Any]): Dictionary of training parameters like 'epochs' and 'batch_size'.\\n\\n    Returns:\\n        Tuple[tf.keras.Model, tf.keras.callbacks.History]: A tuple containing the trained model and the training history object.\\n    \\\"\\\"\\\"\\n\\n    # Example: Configure callbacks for early stopping and saving the best model\\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \\n                                                      patience=10, \\n                                                      restore_best_weights=True)\\n\\n    print(f\\\"--- Starting Training for {model.name} ---\\\")\\n    history = model.fit(\\n        X_train,\\n        y_train,\\n        epochs=training_params.get('epochs', 50),\\n        batch_size=training_params.get('batch_size', 32),\\n        validation_data=(X_val, y_val),\\n        callbacks=[early_stopping],\\n        verbose=1\\n    )\\n    print(f\\\"--- Finished Training for {model.name} ---\\\")\\n\\n    return model, history\\n\",\n  \"documentation\": \"This module orchestrates the model training process. It accepts compiled model architectures and partitioned datasets from upstream modules. Its primary function, `train_model`, executes the core training loop using the `fit` method of the chosen framework. It systematically uses the training set to learn and the validation set to monitor and avoid overfitting, employing callbacks like `EarlyStopping` for efficiency. The module's outputs are the trained model (with its learned weights) and a detailed history of the training process (loss metrics per epoch), which are essential for the subsequent evaluation and visualization stages. This module does NOT define model architectures, preprocess data, or perform the final evaluation on the test set.\"\n}"