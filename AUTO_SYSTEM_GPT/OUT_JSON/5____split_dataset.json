"{\n  \"task_name\": \"split_dataset\",\n  \"module_responsibility\": \"This module is exclusively responsible for partitioning the preprocessed, sequential time series data into training, validation, and test sets. It ensures the split is performed chronologically to maintain the temporal integrity of the data, which is critical for time series forecasting.\",\n  \"model_role_and_purpose\": \"The purpose of this module is to prepare the dataset for the model training and evaluation stages. By creating chronologically distinct subsets (training, validation, test), it simulates a real-world forecasting scenario where a model learns from the past to predict the future. This strict separation prevents data leakage and ensures an unbiased assessment of the model's performance on unseen data.\",\n  \"concrete_tasks\": [\n    {\n      \"task_id\": \"split_dataset_1\",\n      \"task_description\": \"Splits a given sequential dataset (features X and targets y) into training, validation, and test sets based on specified chronological proportions.\",\n      \"function_name\": \"split_sequential_data\",\n      \"inputs\": {\n        \"X\": {\n          \"type\": \"numpy.ndarray\",\n          \"description\": \"A 3D array of input sequences. Shape: (num_samples, sequence_length, num_features).\"\n        },\n        \"y\": {\n          \"type\": \"numpy.ndarray\",\n          \"description\": \"A 1D array of corresponding target values. Shape: (num_samples,).\"\n        },\n        \"split_ratios\": {\n          \"type\": \"tuple\",\n          \"description\": \"A tuple containing the proportions for training, validation, and test sets, e.g., (0.8, 0.1, 0.1).\"\n        }\n      },\n      \"outputs\": {\n        \"datasets\": {\n          \"type\": \"dict\",\n          \"description\": \"A dictionary containing the six split arrays: 'X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'.\"\n        }\n      }\n    }\n  ],\n  \"dependencies\": [\n    \"Preprocess Data\"\n  ],\n  \"constraints\": {\n    \"library_versions_and_configurations\": \"{\\n  \\\"numpy\\\": \\\">=1.20.0\\\"\\n}\",\n    \"error_handling\": \"The module must raise a `ValueError` if the input arrays `X` and `y` do not have the same number of samples (i.e., `X.shape[0] != y.shape[0]`). It should also raise a `ValueError` if the dataset is too small to be split into non-empty training, validation, and test sets.\",\n    \"input_formats_and_data_types\": {\n      \"X\": \"A 3D NumPy array of `float32` type with shape `(num_samples, sequence_length, num_features)`.\",\n      \"y\": \"A 1D NumPy array of `float32` type with shape `(num_samples,)`.\",\n      \"split_ratios\": \"A tuple of three floats that sum to 1.0.\"\n    },\n    \"output_formats_and_data_types\": {\n      \"datasets\": \"A dictionary where keys are strings ('X_train', 'y_train', etc.) and values are the corresponding NumPy arrays of type `float32`. The shape of each array will depend on the split calculation.\"\n    },\n    \"specific_error_handling\": \"In case of a `ValueError` due to insufficient data, the error message should clearly state the number of samples available and the number required to perform the split. Errors should be logged before being raised.\"\n  },\n  \"code_skeleton\": \"import numpy as np\\nfrom typing import Dict, Tuple, List\\n\\ndef split_sequential_data(X: np.ndarray, y: np.ndarray, split_ratios: Tuple[float, float, float] = (0.8, 0.1, 0.1)) -> Dict[str, np.ndarray]:\\n    \\\"\\\"\\\"\\n    Splits time series data chronologically into training, validation, and test sets.\\n\\n    Args:\\n        X (np.ndarray): The input feature sequences.\\n        y (np.ndarray): The target values.\\n        split_ratios (Tuple[float, float, float]): The ratios for train, validation, and test sets.\\n\\n    Returns:\\n        Dict[str, np.ndarray]: A dictionary containing the split datasets.\\n\\n    Raises:\\n        ValueError: If X and y have mismatched lengths or if the dataset is too small.\\n    \\\"\\\"\\\"\\n    if X.shape[0] != y.shape[0]:\\n        raise ValueError(\\\"Input arrays X and y must have the same number of samples.\\\")\\n\\n    total_samples = X.shape[0]\\n    train_ratio, val_ratio, test_ratio = split_ratios\\n\\n    if not np.isclose(train_ratio + val_ratio + test_ratio, 1.0):\\n        raise ValueError(\\\"Split ratios must sum to 1.0.\\\")\\n\\n    train_split_idx = int(total_samples * train_ratio)\\n    val_split_idx = int(total_samples * (train_ratio + val_ratio))\\n\\n    if train_split_idx == 0 or val_split_idx == train_split_idx or val_split_idx == total_samples:\\n        raise ValueError(f\\\"Insufficient data for splitting. Total samples: {total_samples}. Cannot create non-empty train/val/test sets with the given ratios.\\\")\\n\\n    X_train, y_train = X[:train_split_idx], y[:train_split_idx]\\n    X_val, y_val = X[train_split_idx:val_split_idx], y[train_split_idx:val_split_idx]\\n    X_test, y_test = X[val_split_idx:], y[val_split_idx:]\\n\\n    return {\\n        'X_train': X_train,\\n        'y_train': y_train,\\n        'X_val': X_val,\\n        'y_val': y_val,\\n        'X_test': X_test,\\n        'y_test': y_test\\n    }\\n\",\n  \"documentation\": \"This module provides a function `split_sequential_data` to partition time series data. Its primary responsibility is to perform a chronological split, which is essential for preventing data leakage and ensuring the model is evaluated on data that is truly 'future' relative to the training data. The module accepts pre-created sequences (X) and targets (y) from the 'Preprocess Data' module and outputs a dictionary of NumPy arrays for training, validation, and testing. It does not perform any data normalization, feature engineering, or sequence creation. The split ratios are configurable but default to the system-wide requirement of 80% training, 10% validation, and 10% testing.\"\n}"