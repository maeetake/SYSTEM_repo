"{\n  \"task_name\": \"evaluate_model_performance\",\n  \"module_responsibility\": \"This module is exclusively responsible for assessing the performance of the trained deep learning models. It computes key statistical metrics by comparing model predictions against the actual values from the unseen test dataset. It does not perform data loading, preprocessing, model training, or visualization.\",\n  \"model_role_and_purpose\": \"The purpose of this module is to provide a quantitative and objective evaluation of the trained LSTM and Transformer models. By calculating standard error metrics like RMSE and MAE on the test set, it determines the predictive accuracy of each model, which is essential for comparing them and selecting the superior one for deployment.\",\n  \"concrete_tasks\": [\n    {\n      \"task_id\": \"evaluate_model_performance_1\",\n      \"task_description\": \"Generate predictions for the test set using a trained model.\",\n      \"input\": {\n        \"model\": \"A trained model object (TensorFlow/Keras or PyTorch).\",\n        \"X_test\": \"A numpy.ndarray of shape (n_samples, sequence_length, n_features) representing the test features.\"\n      },\n      \"output\": {\n        \"predictions_normalized\": \"A numpy.ndarray of shape (n_samples, 1) containing the model's predictions in the normalized scale [0, 1].\"\n      },\n      \"function_name\": \"make_predictions\"\n    },\n    {\n      \"task_id\": \"evaluate_model_performance_2\",\n      \"task_description\": \"Inverse transform the predicted and actual values back to their original stock price scale.\",\n      \"input\": {\n        \"predictions_normalized\": \"A numpy.ndarray of shape (n_samples, 1).\",\n        \"y_test_normalized\": \"A numpy.ndarray of shape (n_samples, 1) containing the true normalized values.\",\n        \"scaler\": \"A fitted scikit-learn MinMaxScaler object.\"\n      },\n      \"output\": {\n        \"predictions_actual_scale\": \"A numpy.ndarray containing predictions in the original price scale.\",\n        \"y_test_actual_scale\": \"A numpy.ndarray containing true values in the original price scale.\"\n      },\n      \"function_name\": \"inverse_transform_values\"\n    },\n    {\n      \"task_id\": \"evaluate_model_performance_3\",\n      \"task_description\": \"Calculate Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) between the actual and predicted values.\",\n      \"input\": {\n        \"y_true\": \"A numpy.ndarray of actual target values.\",\n        \"y_pred\": \"A numpy.ndarray of predicted values.\"\n      },\n      \"output\": {\n        \"metrics\": \"A dictionary containing the calculated 'RMSE' and 'MAE' as float values. Example: {'RMSE': 10.5, 'MAE': 8.2}\"\n      },\n      \"function_name\": \"calculate_metrics\"\n    }\n  ],\n  \"dependencies\": [\n    \"train_models\",\n    \"split_dataset\"\n  ],\n  \"constraints\": {\n    \"library_versions_and_configurations\": \"Python 3.8+, scikit-learn==1.0+, numpy==1.21+, tensorflow==2.8+ or torch==1.10+\",\n    \"error_handling\": \"The module should handle potential `ValueError` if the shapes of prediction and true value arrays do not match. It should also catch `NotFittedError` if the provided scaler has not been fitted. Errors should be logged, and the function should return an empty dictionary or raise the exception to the caller for handling.\",\n    \"input_formats_and_data_types\": {\n      \"model\": \"A trained `tensorflow.keras.Model` or `torch.nn.Module` object.\",\n      \"X_test\": \"A `numpy.ndarray` of float32 values with shape `(n_test_samples, sequence_length, n_features)`.\",\n      \"y_test\": \"A `numpy.ndarray` of float32 values with shape `(n_test_samples, 1)` containing the normalized target values.\",\n      \"scaler\": \"A `sklearn.preprocessing.MinMaxScaler` object that has been fitted on the training data's closing price column.\"\n    },\n    \"output_formats_and_data_types\": {\n      \"metrics_report\": \"A dictionary where keys are model names ('LSTM', 'Transformer') and values are another dictionary containing 'RMSE' and 'MAE' as floats. Example: {'LSTM': {'RMSE': 10.5, 'MAE': 8.2}, 'Transformer': {'RMSE': 9.8, 'MAE': 7.5}}\",\n      \"predictions_report\": \"A dictionary where keys are model names and values are the corresponding `numpy.ndarray` of predictions in the original price scale, to be used by the visualization module.\"\n    },\n    \"specific_error_handling\": \"If evaluation fails for a specific model, log the model name and the exception details. The system should proceed to evaluate the other model if possible, rather than halting the entire process.\"\n  },\n  \"code_skeleton\": \"import numpy as np\\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\ndef evaluate_model_performance(\\n    model: object, \\n    X_test: np.ndarray, \\n    y_test: np.ndarray, \\n    scaler: MinMaxScaler\\n) -> dict:\\n    \\\"\\\"\\\"\\n    Evaluates a trained model on the test data.\\n\\n    Args:\\n        model: The trained TensorFlow or PyTorch model.\\n        X_test: The test data features.\\n        y_test: The true test data targets (normalized).\\n        scaler: The scaler used to normalize the data.\\n\\n    Returns:\\n        A dictionary containing performance metrics ('RMSE', 'MAE') and predictions.\\n    \\\"\\\"\\\"\\n    try:\\n        # 1. Make predictions\\n        predictions_normalized = model.predict(X_test)\\n\\n        # Reshape y_test if it's a flat array\\n        if y_test.ndim == 1:\\n            y_test = y_test.reshape(-1, 1)\\n\\n        # 2. Inverse transform to get actual price scale\\n        # Create a dummy array matching the scaler's original n_features\\n        dummy_array_pred = np.zeros((len(predictions_normalized), scaler.n_features_in_))\\n        dummy_array_pred[:, -1] = predictions_normalized.ravel() # Assuming 'Close' was the last feature scaled\\n        predictions_actual = scaler.inverse_transform(dummy_array_pred)[:, -1]\\n\\n        dummy_array_true = np.zeros((len(y_test), scaler.n_features_in_))\\n        dummy_array_true[:, -1] = y_test.ravel()\\n        y_test_actual = scaler.inverse_transform(dummy_array_true)[:, -1]\\n\\n        # 3. Calculate metrics\\n        rmse = np.sqrt(mean_squared_error(y_test_actual, predictions_actual))\\n        mae = mean_absolute_error(y_test_actual, predictions_actual)\\n\\n        return {\\n            'metrics': {'RMSE': rmse, 'MAE': mae},\\n            'predictions': predictions_actual,\\n            'actuals': y_test_actual\\n        }\\n    except Exception as e:\\n        print(f\\\"Error during model evaluation: {e}\\\")\\n        return {}\\n\",\n  \"documentation\": \"This module is dedicated to the quantitative assessment of the previously trained models. It takes the trained model objects, the test dataset, and the feature scaler as inputs. Its primary function is to generate predictions on the unseen test data, inverse-transform these predictions from the normalized scale back to the original price scale, and then compute the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) by comparing them against the true values. This module does not handle data splitting or model training. The outputs are a dictionary of performance metrics and the raw predictions, which are consumed by downstream modules for reporting and visualization.\"\n}"