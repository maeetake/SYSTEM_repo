{
    "load_user-provided_data": {
        "task_name": "load_user-provided_data",
        "module_responsibility": "This module is exclusively responsible for loading user-provided data from a specified CSV file path into a pandas DataFrame. Its scope is limited to file I/O and initial structural validation (checking for file existence and required columns). It does not perform any data cleaning, transformation, or analysis.",
        "model_role_and_purpose": "As the first step in the data processing pipeline, this module acts as the sole data ingestion point for the system. Its purpose is to abstract the file loading mechanism and provide a standardized raw data object (a pandas DataFrame) to downstream modules, ensuring a consistent starting point for all subsequent data preprocessing and modeling tasks.",
        "concrete_tasks": [
            {
                "task_id": "load_user-provided_data_1",
                "task_description": "Load data from a CSV file and perform initial validation.",
                "component_type": "function",
                "component_name": "load_data_from_csv",
                "inputs": [
                    {
                        "name": "file_path",
                        "type": "string",
                        "description": "The absolute or relative path to the user-provided CSV file."
                    }
                ],
                "processing_steps": [
                    "Validate that the file exists at the given 'file_path'.",
                    "Use the pandas library to read the CSV file into a DataFrame.",
                    "Check if the loaded DataFrame contains the required columns: 'Date', 'Open', 'High', 'Low', 'Close'.",
                    "Return the DataFrame without any modifications."
                ],
                "outputs": [
                    {
                        "name": "raw_dataframe",
                        "type": "pandas.DataFrame",
                        "description": "A DataFrame containing the raw, unmodified data read from the CSV file."
                    }
                ]
            }
        ],
        "dependencies": [],
        "constraints": {
            "library_versions_and_configurations": "Python 3.8+; pandas>=1.3.0",
            "error_handling": "The module must raise specific exceptions for predictable failure modes. A 'FileNotFoundError' should be raised if the CSV file path is invalid. A 'ValueError' should be raised if the CSV can be found but is empty or cannot be parsed. A 'KeyError' should be raised if the loaded data does not contain all required columns ('Date', 'Open', 'High', 'Low', 'Close'). These exceptions should halt execution and be caught by the parent orchestrator.",
            "input_formats_and_data_types": "The module expects a single string argument ('file_path') pointing to a valid CSV formatted file. The CSV file must be comma-separated and have a header row.",
            "output_formats_and_data_types": "The module must return a single 'pandas.DataFrame' object. This DataFrame will contain the raw data from the CSV. The data types of the columns are not enforced by this module but are expected to be handled by downstream preprocessing modules.",
            "specific_error_handling": "In case of an error, the module should log a detailed error message specifying the file path and the nature of the failure (e.g., 'File not found', 'Missing column: Close') before raising the exception. No data should be returned on failure."
        },
        "code_skeleton": "import pandas as pd\nimport os\n\ndef load_data_from_csv(file_path: str) -> pd.DataFrame:\n    \"\"\"\n    Loads historical stock data from a user-provided CSV file.\n\n    This function reads a CSV file from the specified path, validates its\n    existence and structure, and returns the data as a pandas DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the raw stock data with columns\n                      including 'Date', 'Open', 'High', 'Low', and 'Close'.\n\n    Raises:\n        FileNotFoundError: If the file at 'file_path' does not exist.\n        ValueError: If the CSV file is empty.\n        KeyError: If one of the required columns is missing in the CSV file.\n    \"\"\"\n    required_columns = ['Date', 'Open', 'High', 'Low', 'Close']\n\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"Error: The file was not found at {file_path}\")\n\n    try:\n        data = pd.read_csv(file_path)\n    except Exception as e:\n        raise ValueError(f\"Error parsing CSV file: {e}\")\n\n    if data.empty:\n        raise ValueError(f\"Error: The CSV file at {file_path} is empty.\")\n\n    for col in required_columns:\n        if col not in data.columns:\n            raise KeyError(f\"Error: Required column '{col}' not found in the CSV file.\")\n\n    return data",
        "documentation": "This module is designed as the primary data ingestion point for the stock prediction system. Its function is to load a CSV file specified by the user. It performs two critical initial checks: ensuring the file exists and that it contains the necessary columns ('Date', 'Open', 'High', 'Low', 'Close') for the model. The module a-intentionally does NOT handle any data cleaning, filtering, or preprocessing (such as handling missing values or normalizing data), as those responsibilities belong to the 'preprocess_data' module. Its output is a raw pandas DataFrame, which serves as the standardized input for the next stage of the pipeline."
    },
    "preprocess_data": {
        "task_name": "preprocess_data",
        "module_responsibility": "This module is exclusively responsible for cleaning, transforming, and structuring the raw time-series data. Its duties include handling missing values, scaling features, and creating sequential datasets (input sequences and corresponding targets) suitable for training and evaluating time-series models. It does not load data from files or build/train models.",
        "model_role_and_purpose": "The purpose of this module is to convert the raw, loaded stock data into a clean, normalized, and structured format required by the downstream deep learning models (LSTM and Transformer). Proper preprocessing is critical for model stability, faster convergence, and accurate predictions. This module bridges the gap between raw data and the model training phase.",
        "concrete_tasks": [
            {
                "task_id": "preprocess_data_1",
                "function_name": "handle_missing_values",
                "description": "Checks for and fills any missing values in the OHLC columns of the input DataFrame using the forward fill ('ffill') method.",
                "input": {
                    "df": "pandas.DataFrame containing OHLC data."
                },
                "output": {
                    "df_filled": "pandas.DataFrame with missing values handled."
                }
            },
            {
                "task_id": "preprocess_data_2",
                "function_name": "normalize_data",
                "description": "Normalizes the OHLC feature columns to a range of [0, 1] using scikit-learn's MinMaxScaler. The fitted scaler object is returned for later use in inverse-transforming the predictions.",
                "input": {
                    "df": "pandas.DataFrame with OHLC data."
                },
                "output": {
                    "scaled_data": "numpy.ndarray of shape (n_samples, n_features) with values scaled between 0 and 1.",
                    "scaler": "sklearn.preprocessing.MinMaxScaler object fitted to the data."
                }
            },
            {
                "task_id": "preprocess_data_3",
                "function_name": "create_sequences",
                "description": "Converts the time-series data into input sequences and corresponding target values. It uses a sliding window of 60 days of OHLC data (X) to predict the closing price of the 61st day (y).",
                "input": {
                    "data": "numpy.ndarray of scaled OHLC data.",
                    "sequence_length": "integer, the number of past time steps to use as input features (e.g., 60)."
                },
                "output": {
                    "X": "numpy.ndarray of shape (n_samples, sequence_length, n_features) containing the input sequences.",
                    "y": "numpy.ndarray of shape (n_samples,) containing the target closing prices."
                }
            },
            {
                "task_id": "preprocess_data_4",
                "function_name": "split_data",
                "description": "Splits the sequential data chronologically into training (80%), validation (10%), and test (10%) sets.",
                "input": {
                    "X": "numpy.ndarray of input sequences.",
                    "y": "numpy.ndarray of target values."
                },
                "output": {
                    "splits": "A dictionary containing six numpy arrays: 'X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'."
                }
            }
        ],
        "dependencies": [
            "load_data"
        ],
        "constraints": {
            "library_versions_and_configurations": "Python 3.8+, pandas>=1.3.0, numpy>=1.20.0, scikit-learn>=1.0.0. The MinMaxScaler should use feature_range=(0, 1). The sequence length for creating windows must be fixed at 60.",
            "error_handling": "The module must validate its input DataFrame. It should raise a `ValueError` if required columns ('Open', 'High', 'Low', 'Close') are missing or if the data contains non-numeric types. It should also raise a `ValueError` if the dataset has fewer than 61 rows, making it impossible to create a single sequence.",
            "input_formats_and_data_types": {
                "description": "A single pandas DataFrame provided by the `load_data` module. This DataFrame must contain at least the columns 'Open', 'High', 'Low', 'Close' with data types convertible to numeric (e.g., float64, int64).",
                "schema": {
                    "type": "pandas.DataFrame",
                    "columns": {
                        "Open": "numeric",
                        "High": "numeric",
                        "Low": "numeric",
                        "Close": "numeric"
                    }
                }
            },
            "output_formats_and_data_types": {
                "description": "A dictionary containing the preprocessed and split data, along with the scaler object. The data arrays should be of type numpy.float32 for compatibility with deep learning frameworks.",
                "schema": {
                    "type": "dict",
                    "keys": {
                        "X_train": "numpy.ndarray (shape: [num_train_samples, 60, 4], dtype: float32)",
                        "y_train": "numpy.ndarray (shape: [num_train_samples,], dtype: float32)",
                        "X_val": "numpy.ndarray (shape: [num_val_samples, 60, 4], dtype: float32)",
                        "y_val": "numpy.ndarray (shape: [num_val_samples,], dtype: float32)",
                        "X_test": "numpy.ndarray (shape: [num_test_samples, 60, 4], dtype: float32)",
                        "y_test": "numpy.ndarray (shape: [num_test_samples,], dtype: float32)",
                        "scaler": "sklearn.preprocessing.MinMaxScaler"
                    }
                }
            },
            "specific_error_handling": "Any `ValueError` should be logged with a descriptive message indicating the cause (e.g., 'Input DataFrame is missing required columns', 'Not enough data to create sequences'). If missing values remain at the start of the dataframe after forward fill, those rows should be dropped, and a warning should be logged."
        },
        "code_skeleton": "from typing import Dict, Tuple, List\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\nclass DataPreprocessor:\n    def __init__(self, sequence_length: int = 60):\n        \"\"\"\n        Initializes the preprocessor with a fixed sequence length.\n        \"\"\"\n        self.sequence_length = sequence_length\n        self.scaler = MinMaxScaler(feature_range=(0, 1))\n        self.feature_cols = ['Open', 'High', 'Low', 'Close']\n\n    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fills missing values using forward fill and drops any remaining NaNs.\"\"\"\n        pass\n\n    def _normalize_data(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Scales the feature columns using MinMaxScaler.\"\"\"\n        pass\n\n    def _create_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Creates input sequences and target values from scaled data.\"\"\"\n        pass\n\n    def _split_data(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\n        \"\"\"Splits data chronologically into train, validation, and test sets.\"\"\"\n        pass\n\n    def process(self, df: pd.DataFrame) -> Dict[str, any]:\n        \"\"\"\n        Executes the full preprocessing pipeline.\n\n        Args:\n            df (pd.DataFrame): The raw input DataFrame with OHLC data.\n\n        Returns:\n            Dict[str, any]: A dictionary containing the split datasets (X_train, y_train, etc.)\n                            and the fitted scaler object.\n        \"\"\"\n        # 1. Validate and select features\n        # 2. Handle missing values\n        # 3. Normalize data\n        # 4. Create sequences\n        # 5. Split data\n        # 6. Return dictionary with all artifacts\n        pass\n",
        "documentation": "This module, `preprocess_data`, serves as the data preparation engine for the time-series forecasting system. Its primary function is to take a raw DataFrame of stock prices and transform it into a state ready for model consumption. The process involves several sequential steps: validating input, handling missing data via forward fill, normalizing 'Open', 'High', 'Low', 'Close' features to a [0, 1] range, and structuring the data into sequences of 60 time steps to predict the 61st day's closing price. The module outputs a dictionary containing chronologically split training (80%), validation (10%), and test (10%) sets, along with the scaler used for normalization. This module will not perform data loading or model training; it strictly focuses on data transformation, ensuring a clean and consistent input for the `train_model` module."
    },
    "split_dataset": {
        "task_name": "split_dataset",
        "module_responsibility": "This module is exclusively responsible for partitioning the preprocessed, sequential time series data into training, validation, and test sets. It ensures the split is performed chronologically to maintain the temporal integrity of the data, which is critical for time series forecasting.",
        "model_role_and_purpose": "The purpose of this module is to prepare the dataset for the model training and evaluation stages. By creating chronologically distinct subsets (training, validation, test), it simulates a real-world forecasting scenario where a model learns from the past to predict the future. This strict separation prevents data leakage and ensures an unbiased assessment of the model's performance on unseen data.",
        "concrete_tasks": [
            {
                "task_id": "split_dataset_1",
                "task_description": "Splits a given sequential dataset (features X and targets y) into training, validation, and test sets based on specified chronological proportions.",
                "function_name": "split_sequential_data",
                "inputs": {
                    "X": {
                        "type": "numpy.ndarray",
                        "description": "A 3D array of input sequences. Shape: (num_samples, sequence_length, num_features)."
                    },
                    "y": {
                        "type": "numpy.ndarray",
                        "description": "A 1D array of corresponding target values. Shape: (num_samples,)."
                    },
                    "split_ratios": {
                        "type": "tuple",
                        "description": "A tuple containing the proportions for training, validation, and test sets, e.g., (0.8, 0.1, 0.1)."
                    }
                },
                "outputs": {
                    "datasets": {
                        "type": "dict",
                        "description": "A dictionary containing the six split arrays: 'X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'."
                    }
                }
            }
        ],
        "dependencies": [
            "Preprocess Data"
        ],
        "constraints": {
            "library_versions_and_configurations": "{\n  \"numpy\": \">=1.20.0\"\n}",
            "error_handling": "The module must raise a `ValueError` if the input arrays `X` and `y` do not have the same number of samples (i.e., `X.shape[0] != y.shape[0]`). It should also raise a `ValueError` if the dataset is too small to be split into non-empty training, validation, and test sets.",
            "input_formats_and_data_types": {
                "X": "A 3D NumPy array of `float32` type with shape `(num_samples, sequence_length, num_features)`.",
                "y": "A 1D NumPy array of `float32` type with shape `(num_samples,)`.",
                "split_ratios": "A tuple of three floats that sum to 1.0."
            },
            "output_formats_and_data_types": {
                "datasets": "A dictionary where keys are strings ('X_train', 'y_train', etc.) and values are the corresponding NumPy arrays of type `float32`. The shape of each array will depend on the split calculation."
            },
            "specific_error_handling": "In case of a `ValueError` due to insufficient data, the error message should clearly state the number of samples available and the number required to perform the split. Errors should be logged before being raised."
        },
        "code_skeleton": "import numpy as np\nfrom typing import Dict, Tuple, List\n\ndef split_sequential_data(X: np.ndarray, y: np.ndarray, split_ratios: Tuple[float, float, float] = (0.8, 0.1, 0.1)) -> Dict[str, np.ndarray]:\n    \"\"\"\n    Splits time series data chronologically into training, validation, and test sets.\n\n    Args:\n        X (np.ndarray): The input feature sequences.\n        y (np.ndarray): The target values.\n        split_ratios (Tuple[float, float, float]): The ratios for train, validation, and test sets.\n\n    Returns:\n        Dict[str, np.ndarray]: A dictionary containing the split datasets.\n\n    Raises:\n        ValueError: If X and y have mismatched lengths or if the dataset is too small.\n    \"\"\"\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(\"Input arrays X and y must have the same number of samples.\")\n\n    total_samples = X.shape[0]\n    train_ratio, val_ratio, test_ratio = split_ratios\n\n    if not np.isclose(train_ratio + val_ratio + test_ratio, 1.0):\n        raise ValueError(\"Split ratios must sum to 1.0.\")\n\n    train_split_idx = int(total_samples * train_ratio)\n    val_split_idx = int(total_samples * (train_ratio + val_ratio))\n\n    if train_split_idx == 0 or val_split_idx == train_split_idx or val_split_idx == total_samples:\n        raise ValueError(f\"Insufficient data for splitting. Total samples: {total_samples}. Cannot create non-empty train/val/test sets with the given ratios.\")\n\n    X_train, y_train = X[:train_split_idx], y[:train_split_idx]\n    X_val, y_val = X[train_split_idx:val_split_idx], y[train_split_idx:val_split_idx]\n    X_test, y_test = X[val_split_idx:], y[val_split_idx:]\n\n    return {\n        'X_train': X_train,\n        'y_train': y_train,\n        'X_val': X_val,\n        'y_val': y_val,\n        'X_test': X_test,\n        'y_test': y_test\n    }\n",
        "documentation": "This module provides a function `split_sequential_data` to partition time series data. Its primary responsibility is to perform a chronological split, which is essential for preventing data leakage and ensuring the model is evaluated on data that is truly 'future' relative to the training data. The module accepts pre-created sequences (X) and targets (y) from the 'Preprocess Data' module and outputs a dictionary of NumPy arrays for training, validation, and testing. It does not perform any data normalization, feature engineering, or sequence creation. The split ratios are configurable but default to the system-wide requirement of 80% training, 10% validation, and 10% testing."
    },
    "build_lstm_and_transformer_models": {
        "task_name": "build_lstm_and_transformer_models",
        "module_responsibility": "This module is exclusively responsible for defining and constructing the architectures for an LSTM model and a Transformer model. It does not handle data loading, preprocessing, training, or evaluation. Its output is two separate, uncompiled deep learning model objects.",
        "model_role_and_purpose": "The purpose of this module is to provide the structural blueprints for two distinct time-series forecasting models. These models, once built, will be passed to the training module. By creating both an LSTM and a Transformer, the system can later compare which architecture is more effective for predicting the next-day stock closing price based on the given historical data.",
        "concrete_tasks": [
            {
                "task_name": "Build LSTM Model",
                "description": "Defines and constructs a sequential model using LSTM layers.",
                "implementation_details": "Create a function `build_lstm_model` that takes the input shape (sequence length, number of features) and returns an uncompiled `tf.keras.Model`. The architecture will consist of one or more LSTM layers, followed by Dropout layers for regularization, and a final Dense layer with a single neuron for the output prediction.",
                "inputs": {
                    "input_shape": "A tuple of integers, e.g., (60, 4), representing (sequence_length, num_features)."
                },
                "outputs": {
                    "lstm_model": "An uncompiled `tf.keras.Model` object representing the LSTM architecture."
                }
            },
            {
                "task_name": "Build Transformer Model",
                "description": "Defines and constructs a model using a Transformer encoder architecture.",
                "implementation_details": "Create a function `build_transformer_model` that takes the input shape, number of heads for attention, and other hyperparameters. It will return an uncompiled `tf.keras.Model`. This involves creating helper classes or functions for the Transformer Encoder Block, which includes Multi-Head Self-Attention and Position-wise Feed-Forward Networks, as well as Positional Encoding. The final model will stack these blocks and use a Global Average Pooling layer before a final Dense output layer.",
                "inputs": {
                    "input_shape": "A tuple of integers, e.g., (60, 4), representing (sequence_length, num_features)."
                },
                "outputs": {
                    "transformer_model": "An uncompiled `tf.keras.Model` object representing the Transformer architecture."
                }
            }
        ],
        "dependencies": [
            "preprocess_data"
        ],
        "constraints": {
            "library_versions_and_configurations": "Python: 3.8+, TensorFlow: 2.10+, NumPy: 1.21+. Models must be implemented as `tf.keras.Model` subclasses or using the Keras Sequential/Functional API. No compilation of models should occur in this module.",
            "error_handling": "Functions should validate the `input_shape` parameter. If `input_shape` is not a tuple of two positive integers, a `ValueError` should be raised with a descriptive message. Invalid hyperparameter values (e.g., non-positive number of heads) should also raise a `ValueError`.",
            "input_formats_and_data_types": "The input to the functions in this module must be `input_shape` (a Python tuple of `int`). There are no direct data file inputs.",
            "output_formats_and_data_types": "The outputs must be two distinct `tf.keras.Model` objects, one for the LSTM and one for the Transformer. These models must be uncompiled, ready for the training module to add a loss function and optimizer.",
            "specific_error_handling": "Errors related to invalid model architecture parameters (e.g., invalid input shape) should be logged as critical errors, and the execution should be halted, as subsequent training cannot proceed without valid models."
        },
        "code_skeleton": "import tensorflow as tf\nfrom tensorflow.keras import layers\n\nclass TransformerEncoderBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerEncoderBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            layers.Dense(ff_dim, activation=\"relu\"),\n            layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\nclass PositionalEncoding(layers.Layer):\n    # Implementation for positional encoding\n    pass\n\ndef build_lstm_model(input_shape: tuple) -> tf.keras.Model:\n    \"\"\"\n    Builds an LSTM model architecture.\n\n    Args:\n        input_shape: A tuple specifying the input shape (sequence_length, num_features).\n\n    Returns:\n        An uncompiled tf.keras.Model for the LSTM.\n    \"\"\"\n    if not (isinstance(input_shape, tuple) and len(input_shape) == 2 and all(isinstance(dim, int) and dim > 0 for dim in input_shape)):\n        raise ValueError(f\"Invalid input_shape: {input_shape}. Expected a tuple of two positive integers.\")\n\n    model = tf.keras.Sequential([\n        layers.Input(shape=input_shape),\n        layers.LSTM(units=50, return_sequences=True),\n        layers.Dropout(0.2),\n        layers.LSTM(units=50, return_sequences=False),\n        layers.Dropout(0.2),\n        layers.Dense(units=25, activation='relu'),\n        layers.Dense(units=1)\n    ])\n    return model\n\ndef build_transformer_model(input_shape: tuple, head_size: int = 256, num_heads: int = 4, ff_dim: int = 4, num_transformer_blocks: int = 4, mlp_units: list = [128], dropout: float = 0.25, mlp_dropout: float = 0.4) -> tf.keras.Model:\n    \"\"\"\n    Builds a Transformer model architecture.\n\n    Args:\n        input_shape: A tuple specifying the input shape (sequence_length, num_features).\n        head_size: Dimensionality of the model.\n        num_heads: Number of attention heads.\n        ff_dim: Hidden layer size in feed forward network inside transformer.\n        num_transformer_blocks: Number of encoder blocks to stack.\n        mlp_units: A list of integers for the final MLP layer sizes.\n        dropout: Dropout rate for the transformer block.\n        mlp_dropout: Dropout rate for the final MLP layers.\n\n    Returns:\n        An uncompiled tf.keras.Model for the Transformer.\n    \"\"\"\n    if not (isinstance(input_shape, tuple) and len(input_shape) == 2 and all(isinstance(dim, int) and dim > 0 for dim in input_shape)):\n        raise ValueError(f\"Invalid input_shape: {input_shape}. Expected a tuple of two positive integers.\")\n\n    inputs = tf.keras.Input(shape=input_shape)\n    x = PositionalEncoding(input_shape[0], input_shape[1])(inputs) # Hypothetical implementation\n    x = layers.Dense(head_size)(x) # Project features to model dimension\n\n    for _ in range(num_transformer_blocks):\n        x = TransformerEncoderBlock(head_size, num_heads, ff_dim, dropout)(x)\n\n    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"relu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n    outputs = layers.Dense(1)(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=outputs)",
        "documentation": "This module provides functions to construct the neural network architectures for time-series prediction. It contains `build_lstm_model` and `build_transformer_model`. Each function takes the data input shape (determined by the preprocessing module) and returns a corresponding uncompiled TensorFlow Keras model. This module's scope is strictly limited to model definition; it does not handle data, training, compilation (i.e., setting optimizers or loss functions), or evaluation. The returned models are intended to be consumed by the `train_models` module, which will then compile and train them."
    },
    "train_models": {
        "task_name": "train_models",
        "module_responsibility": "This module is responsible for the training loop of the deep learning models. It takes the compiled model architectures (LSTM and Transformer) and the preprocessed, split datasets (training and validation) as input. Its sole focus is to execute the training process, monitor performance on the validation set, and save the best-performing model weights and training history. It does not handle data loading, preprocessing, model definition, or final test set evaluation.",
        "model_role_and_purpose": "The purpose of this module is to use the training dataset to teach the LSTM and Transformer models the underlying patterns of stock price movements. By iteratively adjusting their internal weights based on a loss function, the models learn to predict the next day's closing price. The validation set is used concurrently to prevent overfitting, ensuring that the models generalize well to data they have not been trained on. This module produces the two core artifacts: the trained models.",
        "concrete_tasks": [
            {
                "task_id": "train_models_1",
                "name": "train_model",
                "description": "A function to train a given model on the training data, using the validation data for performance monitoring and to save the best model.",
                "inputs": [
                    {
                        "name": "model",
                        "type": "tensorflow.keras.Model or torch.nn.Module",
                        "description": "The compiled, un-trained model architecture (either LSTM or Transformer)."
                    },
                    {
                        "name": "X_train",
                        "type": "numpy.ndarray",
                        "format": "3D array of shape (num_samples, sequence_length, num_features) with float32 values."
                    },
                    {
                        "name": "y_train",
                        "type": "numpy.ndarray",
                        "format": "1D array of shape (num_samples,) with float32 values."
                    },
                    {
                        "name": "X_val",
                        "type": "numpy.ndarray",
                        "format": "3D array of shape (num_samples, sequence_length, num_features) with float32 values."
                    },
                    {
                        "name": "y_val",
                        "type": "numpy.ndarray",
                        "format": "1D array of shape (num_samples,) with float32 values."
                    },
                    {
                        "name": "training_params",
                        "type": "dict",
                        "description": "A dictionary containing hyperparameters for training, e.g., {'epochs': 50, 'batch_size': 32}."
                    }
                ],
                "outputs": [
                    {
                        "name": "trained_model",
                        "type": "tensorflow.keras.Model or torch.nn.Module",
                        "description": "The model with the weights that achieved the best performance on the validation set."
                    },
                    {
                        "name": "training_history",
                        "type": "dict",
                        "description": "A dictionary or history object containing lists of training and validation loss values for each epoch. Example: {'loss': [0.1, 0.08, ...], 'val_loss': [0.12, 0.09, ...]}"
                    }
                ]
            }
        ],
        "dependencies": [
            "split_data",
            "build_models"
        ],
        "constraints": {
            "library_versions_and_configurations": "Use TensorFlow >= 2.10 or PyTorch >= 1.12. Training should be configured to use a GPU if available to accelerate the process. The Adam optimizer is recommended.",
            "error_handling": "The module must handle potential training errors. `ValueError` should be raised for mismatched input data shapes. Framework-specific exceptions (e.g., TensorFlow's `tf.errors.ResourceExhaustedError` for out-of-memory issues) should be caught, logged with details, and can suggest reducing batch size or model complexity. The function should not suppress exceptions but let them propagate to the caller for system-level handling.",
            "input_formats_and_data_types": "Inputs (`X_train`, `y_train`, `X_val`, `y_val`) must be NumPy arrays of type `numpy.float32`. The model input must be a compiled instance of `tf.keras.Model` or a `torch.nn.Module` with a defined forward pass.",
            "output_formats_and_data_types": "The primary outputs are the trained model object and a history object/dictionary. The trained model should be saved to disk in the framework's standard format (e.g., .h5 for Keras). The history object's data (training/validation loss) must be accessible for the `visualize_results` module.",
            "specific_error_handling": "Log start and end of training for each model. At each epoch, log the training and validation loss to a file or standard output for real-time monitoring. Implement EarlyStopping to halt training if validation loss does not improve for a set number of epochs (e.g., 10), preventing wasted computation and overfitting."
        },
        "code_skeleton": "import tensorflow as tf\nfrom typing import Tuple, Dict, Any\nimport numpy as np\n\ndef train_model(model: tf.keras.Model, \n                X_train: np.ndarray, \n                y_train: np.ndarray, \n                X_val: np.ndarray, \n                y_val: np.ndarray, \n                training_params: Dict[str, Any]) -> Tuple[tf.keras.Model, tf.keras.callbacks.History]:\n    \"\"\"\n    Trains a given Keras model on the provided training and validation datasets.\n\n    Args:\n        model (tf.keras.Model): The compiled Keras model to be trained.\n        X_train (np.ndarray): Training data features.\n        y_train (np.ndarray): Training data targets.\n        X_val (np.ndarray): Validation data features.\n        y_val (np.ndarray): Validation data targets.\n        training_params (Dict[str, Any]): Dictionary of training parameters like 'epochs' and 'batch_size'.\n\n    Returns:\n        Tuple[tf.keras.Model, tf.keras.callbacks.History]: A tuple containing the trained model and the training history object.\n    \"\"\"\n\n    # Example: Configure callbacks for early stopping and saving the best model\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                                      patience=10, \n                                                      restore_best_weights=True)\n\n    print(f\"--- Starting Training for {model.name} ---\")\n    history = model.fit(\n        X_train,\n        y_train,\n        epochs=training_params.get('epochs', 50),\n        batch_size=training_params.get('batch_size', 32),\n        validation_data=(X_val, y_val),\n        callbacks=[early_stopping],\n        verbose=1\n    )\n    print(f\"--- Finished Training for {model.name} ---\")\n\n    return model, history\n",
        "documentation": "This module orchestrates the model training process. It accepts compiled model architectures and partitioned datasets from upstream modules. Its primary function, `train_model`, executes the core training loop using the `fit` method of the chosen framework. It systematically uses the training set to learn and the validation set to monitor and avoid overfitting, employing callbacks like `EarlyStopping` for efficiency. The module's outputs are the trained model (with its learned weights) and a detailed history of the training process (loss metrics per epoch), which are essential for the subsequent evaluation and visualization stages. This module does NOT define model architectures, preprocess data, or perform the final evaluation on the test set."
    },
    "evaluate_model_performance": {
        "task_name": "evaluate_model_performance",
        "module_responsibility": "This module is exclusively responsible for assessing the performance of the trained deep learning models. It computes key statistical metrics by comparing model predictions against the actual values from the unseen test dataset. It does not perform data loading, preprocessing, model training, or visualization.",
        "model_role_and_purpose": "The purpose of this module is to provide a quantitative and objective evaluation of the trained LSTM and Transformer models. By calculating standard error metrics like RMSE and MAE on the test set, it determines the predictive accuracy of each model, which is essential for comparing them and selecting the superior one for deployment.",
        "concrete_tasks": [
            {
                "task_id": "evaluate_model_performance_1",
                "task_description": "Generate predictions for the test set using a trained model.",
                "input": {
                    "model": "A trained model object (TensorFlow/Keras or PyTorch).",
                    "X_test": "A numpy.ndarray of shape (n_samples, sequence_length, n_features) representing the test features."
                },
                "output": {
                    "predictions_normalized": "A numpy.ndarray of shape (n_samples, 1) containing the model's predictions in the normalized scale [0, 1]."
                },
                "function_name": "make_predictions"
            },
            {
                "task_id": "evaluate_model_performance_2",
                "task_description": "Inverse transform the predicted and actual values back to their original stock price scale.",
                "input": {
                    "predictions_normalized": "A numpy.ndarray of shape (n_samples, 1).",
                    "y_test_normalized": "A numpy.ndarray of shape (n_samples, 1) containing the true normalized values.",
                    "scaler": "A fitted scikit-learn MinMaxScaler object."
                },
                "output": {
                    "predictions_actual_scale": "A numpy.ndarray containing predictions in the original price scale.",
                    "y_test_actual_scale": "A numpy.ndarray containing true values in the original price scale."
                },
                "function_name": "inverse_transform_values"
            },
            {
                "task_id": "evaluate_model_performance_3",
                "task_description": "Calculate Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) between the actual and predicted values.",
                "input": {
                    "y_true": "A numpy.ndarray of actual target values.",
                    "y_pred": "A numpy.ndarray of predicted values."
                },
                "output": {
                    "metrics": "A dictionary containing the calculated 'RMSE' and 'MAE' as float values. Example: {'RMSE': 10.5, 'MAE': 8.2}"
                },
                "function_name": "calculate_metrics"
            }
        ],
        "dependencies": [
            "train_models",
            "split_dataset"
        ],
        "constraints": {
            "library_versions_and_configurations": "Python 3.8+, scikit-learn==1.0+, numpy==1.21+, tensorflow==2.8+ or torch==1.10+",
            "error_handling": "The module should handle potential `ValueError` if the shapes of prediction and true value arrays do not match. It should also catch `NotFittedError` if the provided scaler has not been fitted. Errors should be logged, and the function should return an empty dictionary or raise the exception to the caller for handling.",
            "input_formats_and_data_types": {
                "model": "A trained `tensorflow.keras.Model` or `torch.nn.Module` object.",
                "X_test": "A `numpy.ndarray` of float32 values with shape `(n_test_samples, sequence_length, n_features)`.",
                "y_test": "A `numpy.ndarray` of float32 values with shape `(n_test_samples, 1)` containing the normalized target values.",
                "scaler": "A `sklearn.preprocessing.MinMaxScaler` object that has been fitted on the training data's closing price column."
            },
            "output_formats_and_data_types": {
                "metrics_report": "A dictionary where keys are model names ('LSTM', 'Transformer') and values are another dictionary containing 'RMSE' and 'MAE' as floats. Example: {'LSTM': {'RMSE': 10.5, 'MAE': 8.2}, 'Transformer': {'RMSE': 9.8, 'MAE': 7.5}}",
                "predictions_report": "A dictionary where keys are model names and values are the corresponding `numpy.ndarray` of predictions in the original price scale, to be used by the visualization module."
            },
            "specific_error_handling": "If evaluation fails for a specific model, log the model name and the exception details. The system should proceed to evaluate the other model if possible, rather than halting the entire process."
        },
        "code_skeleton": "import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef evaluate_model_performance(\n    model: object, \n    X_test: np.ndarray, \n    y_test: np.ndarray, \n    scaler: MinMaxScaler\n) -> dict:\n    \"\"\"\n    Evaluates a trained model on the test data.\n\n    Args:\n        model: The trained TensorFlow or PyTorch model.\n        X_test: The test data features.\n        y_test: The true test data targets (normalized).\n        scaler: The scaler used to normalize the data.\n\n    Returns:\n        A dictionary containing performance metrics ('RMSE', 'MAE') and predictions.\n    \"\"\"\n    try:\n        # 1. Make predictions\n        predictions_normalized = model.predict(X_test)\n\n        # Reshape y_test if it's a flat array\n        if y_test.ndim == 1:\n            y_test = y_test.reshape(-1, 1)\n\n        # 2. Inverse transform to get actual price scale\n        # Create a dummy array matching the scaler's original n_features\n        dummy_array_pred = np.zeros((len(predictions_normalized), scaler.n_features_in_))\n        dummy_array_pred[:, -1] = predictions_normalized.ravel() # Assuming 'Close' was the last feature scaled\n        predictions_actual = scaler.inverse_transform(dummy_array_pred)[:, -1]\n\n        dummy_array_true = np.zeros((len(y_test), scaler.n_features_in_))\n        dummy_array_true[:, -1] = y_test.ravel()\n        y_test_actual = scaler.inverse_transform(dummy_array_true)[:, -1]\n\n        # 3. Calculate metrics\n        rmse = np.sqrt(mean_squared_error(y_test_actual, predictions_actual))\n        mae = mean_absolute_error(y_test_actual, predictions_actual)\n\n        return {\n            'metrics': {'RMSE': rmse, 'MAE': mae},\n            'predictions': predictions_actual,\n            'actuals': y_test_actual\n        }\n    except Exception as e:\n        print(f\"Error during model evaluation: {e}\")\n        return {}\n",
        "documentation": "This module is dedicated to the quantitative assessment of the previously trained models. It takes the trained model objects, the test dataset, and the feature scaler as inputs. Its primary function is to generate predictions on the unseen test data, inverse-transform these predictions from the normalized scale back to the original price scale, and then compute the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) by comparing them against the true values. This module does not handle data splitting or model training. The outputs are a dictionary of performance metrics and the raw predictions, which are consumed by downstream modules for reporting and visualization."
    },
    "visualize_prediction_results": {
        "task_name": "visualize_prediction_results",
        "module_responsibility": "This module is exclusively responsible for generating graphical representations of the model's prediction results. It takes the actual and predicted time series data for the test set and creates visualizations to allow for qualitative assessment. It does not perform any data preprocessing, model evaluation, or metric calculation.",
        "model_role_and_purpose": "The purpose of this module is to provide a clear, intuitive visual comparison between the actual stock closing prices and the prices predicted by the LSTM and Transformer models. By overlaying these time series on a single graph, stakeholders can quickly assess the models' performance in capturing price trends, volatility, and turning points, which complements the quantitative metrics (RMSE, MAE) calculated by the evaluation module.",
        "concrete_tasks": [
            {
                "task_name": "generate_prediction_plot",
                "description": "Creates and saves a time-series plot comparing actual test set prices with the predicted prices from both the LSTM and Transformer models.",
                "interface": "function",
                "function_name": "plot_predictions",
                "inputs": {
                    "actual_prices": {
                        "type": "pandas.Series",
                        "description": "A pandas Series containing the true closing prices for the test period."
                    },
                    "predicted_prices_lstm": {
                        "type": "numpy.ndarray",
                        "description": "An array of predicted closing prices from the LSTM model for the test period."
                    },
                    "predicted_prices_transformer": {
                        "type": "numpy.ndarray",
                        "description": "An array of predicted closing prices from the Transformer model for the test period."
                    },
                    "dates": {
                        "type": "pandas.Series",
                        "description": "A pandas Series of datetime objects corresponding to the test period, to be used as the x-axis."
                    },
                    "output_path": {
                        "type": "str",
                        "description": "The file path (including filename and extension, e.g., 'results/prediction_comparison.png') where the generated plot will be saved."
                    }
                },
                "outputs": {
                    "saved_plot_path": {
                        "type": "str",
                        "description": "Returns the absolute path to the saved image file upon successful generation."
                    }
                },
                "processing_steps": [
                    "Initialize a plot figure using a library like Matplotlib.",
                    "Plot the 'actual_prices' against the 'dates' series as the ground truth line.",
                    "Overlay the 'predicted_prices_lstm' on the same plot.",
                    "Overlay the 'predicted_prices_transformer' on the same plot.",
                    "Set a clear title for the plot (e.g., 'Stock Price Prediction: Actual vs. Predicted').",
                    "Label the x-axis ('Date') and y-axis ('Closing Price').",
                    "Add a legend to distinguish between the actual, LSTM, and Transformer data series.",
                    "Save the plot to the specified 'output_path'.",
                    "Return the path of the saved file."
                ]
            }
        ],
        "dependencies": [
            "evaluate_model"
        ],
        "constraints": {
            "library_versions_and_configurations": "matplotlib>=3.5.0, pandas>=1.3.0, numpy>=1.21.0. The plot style should be consistent, using a predefined theme for clarity and professional appearance (e.g., 'seaborn-v0_8-whitegrid').",
            "error_handling": "The module must validate that all input arrays/series ('actual_prices', 'predicted_prices_lstm', 'predicted_prices_transformer', 'dates') have the same length. If not, it must raise a ValueError with a descriptive message. A FileNotFoundError or IOError should be raised if the directory in 'output_path' does not exist or if the file cannot be written due to permissions.",
            "input_formats_and_data_types": "Inputs must strictly adhere to the types specified in 'concrete_tasks'. 'actual_prices' and 'dates' must be pandas Series, and 'predicted_prices_lstm' / 'predicted_prices_transformer' must be NumPy arrays. The data within these structures should be numeric (float or integer).",
            "output_formats_and_data_types": "The function's return value must be a string representing a valid file path. The generated plot file format should be configurable but default to PNG for high-quality, lossless compression. The plot must include a title, legend, and axis labels to be considered valid.",
            "specific_error_handling": "Any exception must be logged with a timestamp, the module name ('visualize_prediction_results'), and a stack trace. For example, 'ERROR:visualize_prediction_results: Input arrays have mismatched lengths. Actual: 100, LSTM: 99.'"
        },
        "code_skeleton": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom typing import Union\n\ndef plot_predictions(\n    actual_prices: pd.Series,\n    predicted_prices_lstm: np.ndarray,\n    predicted_prices_transformer: np.ndarray,\n    dates: pd.Series,\n    output_path: str\n) -> str:\n    \"\"\"\n    Generates and saves a time-series plot comparing actual and predicted stock prices.\n\n    Args:\n        actual_prices (pd.Series): The true closing prices for the test set.\n        predicted_prices_lstm (np.ndarray): Predictions from the LSTM model.\n        predicted_prices_transformer (np.ndarray): Predictions from the Transformer model.\n        dates (pd.Series): The corresponding dates for the test set.\n        output_path (str): The file path to save the plot image.\n\n    Returns:\n        str: The path where the plot was saved.\n\n    Raises:\n        ValueError: If input series/arrays have mismatched lengths.\n        IOError: If the plot cannot be saved to the specified path.\n    \"\"\"\n    # 1. Input validation\n    if not (len(actual_prices) == len(predicted_prices_lstm) == len(predicted_prices_transformer) == len(dates)):\n        raise ValueError(\"Input data series and arrays must have the same length.\")\n\n    # 2. Plotting logic\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(15, 7))\n\n    plt.plot(dates, actual_prices, color='blue', label='Actual Price')\n    plt.plot(dates, predicted_prices_lstm, color='orange', linestyle='--', label='LSTM Prediction')\n    plt.plot(dates, predicted_prices_transformer, color='green', linestyle='-.', label='Transformer Prediction')\n\n    # 3. Formatting\n    plt.title('NVIDIA Stock Price Prediction: Actual vs. Predicted', fontsize=16)\n    plt.xlabel('Date', fontsize=12)\n    plt.ylabel('Closing Price (USD)', fontsize=12)\n    plt.legend()\n    plt.grid(True)\n    \n    # 4. Save and return path\n    try:\n        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n        plt.close()\n        # Consider returning an absolute path\n        import os\n        return os.path.abspath(output_path)\n    except Exception as e:\n        # Log the error here\n        raise IOError(f\"Could not save plot to {output_path}. Reason: {e}\")\n",
        "documentation": "This module provides the `plot_predictions` function to visualize time-series prediction results. It is designed to be called after model evaluation, taking the actual prices and the predictions from both the LSTM and Transformer models as input. Its sole responsibility is to generate a high-quality, clearly-labeled plot and save it to a specified file location. The module deliberately avoids any form of data manipulation (e.g., inverse scaling) or metric computation, as these tasks are handled by upstream modules ('preprocess_data' and 'evaluate_model' respectively). This clear separation of concerns ensures maintainability and modularity within the system."
    },
    "visualize_training_history": {
        "task_name": "visualize_training_history",
        "module_responsibility": "This module is exclusively responsible for creating visual representations of the model training process. It generates plots that show the progression of training and validation loss over epochs for both the LSTM and Transformer models. Its scope is limited to data visualization and does not include model training, evaluation, or data processing.",
        "model_role_and_purpose": "The purpose of this module is to provide a diagnostic tool to analyze the training phase of the deep learning models. By plotting training and validation loss curves on the same graph, it helps in identifying model behaviors such as overfitting (validation loss increases while training loss decreases), underfitting (both losses remain high), or ideal convergence. These visualizations are essential for validating the training strategy and for inclusion in final reports.",
        "concrete_tasks": [
            {
                "task_id": "visualize_training_history_1",
                "description": "Generates and saves a plot of the training and validation loss for a given model's training history.",
                "interface": "plot_and_save_history(model_history, model_name, output_path)",
                "inputs": {
                    "model_history": {
                        "type": "dict",
                        "description": "A dictionary-like object containing the training history. Must contain keys 'loss' and 'val_loss', each mapping to a list of floating-point numbers representing the loss at each epoch. This object is typically returned by the Keras `model.fit()` method."
                    },
                    "model_name": {
                        "type": "str",
                        "description": "The name of the model (e.g., 'LSTM', 'Transformer') to be used in the plot title."
                    },
                    "output_path": {
                        "type": "str",
                        "description": "The file path where the generated plot image will be saved (e.g., './plots/lstm_loss_history.png')."
                    }
                },
                "outputs": {
                    "status": {
                        "type": "str",
                        "description": "Returns the path of the saved image file upon successful creation."
                    }
                }
            }
        ],
        "dependencies": [
            "train_models"
        ],
        "constraints": {
            "library_versions_and_configurations": "Use Python 3.8+. Required libraries: matplotlib (version 3.5.0 or newer). Plots must be saved in PNG format with a resolution of at least 300 DPI.",
            "error_handling": "The module should handle exceptions gracefully. If 'model_history' is missing the required 'loss' or 'val_loss' keys, a KeyError should be raised. If the 'output_path' is invalid or not writable, an IOError or OSError should be caught and logged, preventing the pipeline from crashing.",
            "input_formats_and_data_types": "The input 'model_history' must be a dictionary or a Keras History object with 'loss' and 'val_loss' keys, where values are lists of floats. 'model_name' and 'output_path' must be valid non-empty strings.",
            "output_formats_and_data_types": "The module's primary output is the side effect of saving image files to the disk in PNG format. The function should return a string containing the path to the saved file.",
            "specific_error_handling": "Any I/O errors during file saving must be logged with a timestamp and error message. Invalid input data should result in a ValueError or KeyError with a message indicating the exact problem (e.g., 'Missing val_loss key in history object')."
        },
        "code_skeleton": "import matplotlib.pyplot as plt\nfrom typing import Dict, List\n\ndef plot_and_save_history(model_history: Dict[str, List[float]], model_name: str, output_path: str) -> str:\n    \"\"\"\n    Plots the training and validation loss from a model's training history and saves it to a file.\n\n    Args:\n        model_history (Dict[str, List[float]]): A dictionary from Keras history, \n                                                 containing 'loss' and 'val_loss' lists.\n        model_name (str): The name of the model, used for the plot title (e.g., 'LSTM').\n        output_path (str): The path to save the plot image file.\n\n    Returns:\n        str: The path where the plot was saved.\n\n    Raises:\n        KeyError: If 'loss' or 'val_loss' keys are not in model_history.\n        IOError: If the file cannot be saved to the specified output_path.\n    \"\"\"\n    if 'loss' not in model_history or 'val_loss' not in model_history:\n        raise KeyError(\"The 'model_history' dictionary must contain 'loss' and 'val_loss' keys.\")\n\n    plt.figure(figsize=(10, 6))\n    plt.plot(model_history['loss'], label='Training Loss')\n    plt.plot(model_history['val_loss'], label='Validation Loss')\n    plt.title(f'{model_name} Model - Training & Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    try:\n        plt.savefig(output_path, dpi=300)\n        plt.close()\n    except IOError as e:\n        print(f\"Error saving plot to {output_path}: {e}\")\n        raise\n\n    return output_path\n",
        "documentation": "This module provides the `plot_and_save_history` function to visualize the training history of a deep learning model. It is designed to be called after the `train_models` module completes. It accepts a history object (containing loss and validation loss per epoch), a model name for titling, and a file path for saving the output. The module's responsibility is strictly visualization; it does not perform any data manipulation or model training. The output is two separate plot images, one for the LSTM model and one for the Transformer model, saved to the filesystem. This visual feedback is crucial for assessing model performance and diagnosing training issues like overfitting."
    }
}