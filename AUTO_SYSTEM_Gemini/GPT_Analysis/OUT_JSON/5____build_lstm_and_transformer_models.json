"{\n  \"task_name\": \"build_lstm_and_transformer_models\",\n  \"module_responsibility\": \"This module is exclusively responsible for defining and constructing the architectures for an LSTM model and a Transformer model. It does not handle data loading, preprocessing, training, or evaluation. Its output is two separate, uncompiled deep learning model objects.\",\n  \"model_role_and_purpose\": \"The purpose of this module is to provide the structural blueprints for two distinct time-series forecasting models. These models, once built, will be passed to the training module. By creating both an LSTM and a Transformer, the system can later compare which architecture is more effective for predicting the next-day stock closing price based on the given historical data.\",\n  \"concrete_tasks\": [\n    {\n      \"task_name\": \"Build LSTM Model\",\n      \"description\": \"Defines and constructs a sequential model using LSTM layers.\",\n      \"implementation_details\": \"Create a function `build_lstm_model` that takes the input shape (sequence length, number of features) and returns an uncompiled `tf.keras.Model`. The architecture will consist of one or more LSTM layers, followed by Dropout layers for regularization, and a final Dense layer with a single neuron for the output prediction.\",\n      \"inputs\": {\n        \"input_shape\": \"A tuple of integers, e.g., (60, 4), representing (sequence_length, num_features).\"\n      },\n      \"outputs\": {\n        \"lstm_model\": \"An uncompiled `tf.keras.Model` object representing the LSTM architecture.\"\n      }\n    },\n    {\n      \"task_name\": \"Build Transformer Model\",\n      \"description\": \"Defines and constructs a model using a Transformer encoder architecture.\",\n      \"implementation_details\": \"Create a function `build_transformer_model` that takes the input shape, number of heads for attention, and other hyperparameters. It will return an uncompiled `tf.keras.Model`. This involves creating helper classes or functions for the Transformer Encoder Block, which includes Multi-Head Self-Attention and Position-wise Feed-Forward Networks, as well as Positional Encoding. The final model will stack these blocks and use a Global Average Pooling layer before a final Dense output layer.\",\n      \"inputs\": {\n        \"input_shape\": \"A tuple of integers, e.g., (60, 4), representing (sequence_length, num_features).\"\n      },\n      \"outputs\": {\n        \"transformer_model\": \"An uncompiled `tf.keras.Model` object representing the Transformer architecture.\"\n      }\n    }\n  ],\n  \"dependencies\": [\n    \"preprocess_data\"\n  ],\n  \"constraints\": {\n    \"library_versions_and_configurations\": \"Python: 3.8+, TensorFlow: 2.10+, NumPy: 1.21+. Models must be implemented as `tf.keras.Model` subclasses or using the Keras Sequential/Functional API. No compilation of models should occur in this module.\",\n    \"error_handling\": \"Functions should validate the `input_shape` parameter. If `input_shape` is not a tuple of two positive integers, a `ValueError` should be raised with a descriptive message. Invalid hyperparameter values (e.g., non-positive number of heads) should also raise a `ValueError`.\",\n    \"input_formats_and_data_types\": \"The input to the functions in this module must be `input_shape` (a Python tuple of `int`). There are no direct data file inputs.\",\n    \"output_formats_and_data_types\": \"The outputs must be two distinct `tf.keras.Model` objects, one for the LSTM and one for the Transformer. These models must be uncompiled, ready for the training module to add a loss function and optimizer.\",\n    \"specific_error_handling\": \"Errors related to invalid model architecture parameters (e.g., invalid input shape) should be logged as critical errors, and the execution should be halted, as subsequent training cannot proceed without valid models.\"\n  },\n  \"code_skeleton\": \"import tensorflow as tf\\nfrom tensorflow.keras import layers\\n\\nclass TransformerEncoderBlock(layers.Layer):\\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\\n        super(TransformerEncoderBlock, self).__init__()\\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\\n        self.ffn = tf.keras.Sequential([\\n            layers.Dense(ff_dim, activation=\\\"relu\\\"),\\n            layers.Dense(embed_dim),\\n        ])\\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\\n        self.dropout1 = layers.Dropout(rate)\\n        self.dropout2 = layers.Dropout(rate)\\n\\n    def call(self, inputs, training):\\n        attn_output = self.att(inputs, inputs)\\n        attn_output = self.dropout1(attn_output, training=training)\\n        out1 = self.layernorm1(inputs + attn_output)\\n        ffn_output = self.ffn(out1)\\n        ffn_output = self.dropout2(ffn_output, training=training)\\n        return self.layernorm2(out1 + ffn_output)\\n\\nclass PositionalEncoding(layers.Layer):\\n    # Implementation for positional encoding\\n    pass\\n\\ndef build_lstm_model(input_shape: tuple) -> tf.keras.Model:\\n    \\\"\\\"\\\"\\n    Builds an LSTM model architecture.\\n\\n    Args:\\n        input_shape: A tuple specifying the input shape (sequence_length, num_features).\\n\\n    Returns:\\n        An uncompiled tf.keras.Model for the LSTM.\\n    \\\"\\\"\\\"\\n    if not (isinstance(input_shape, tuple) and len(input_shape) == 2 and all(isinstance(dim, int) and dim > 0 for dim in input_shape)):\\n        raise ValueError(f\\\"Invalid input_shape: {input_shape}. Expected a tuple of two positive integers.\\\")\\n\\n    model = tf.keras.Sequential([\\n        layers.Input(shape=input_shape),\\n        layers.LSTM(units=50, return_sequences=True),\\n        layers.Dropout(0.2),\\n        layers.LSTM(units=50, return_sequences=False),\\n        layers.Dropout(0.2),\\n        layers.Dense(units=25, activation='relu'),\\n        layers.Dense(units=1)\\n    ])\\n    return model\\n\\ndef build_transformer_model(input_shape: tuple, head_size: int = 256, num_heads: int = 4, ff_dim: int = 4, num_transformer_blocks: int = 4, mlp_units: list = [128], dropout: float = 0.25, mlp_dropout: float = 0.4) -> tf.keras.Model:\\n    \\\"\\\"\\\"\\n    Builds a Transformer model architecture.\\n\\n    Args:\\n        input_shape: A tuple specifying the input shape (sequence_length, num_features).\\n        head_size: Dimensionality of the model.\\n        num_heads: Number of attention heads.\\n        ff_dim: Hidden layer size in feed forward network inside transformer.\\n        num_transformer_blocks: Number of encoder blocks to stack.\\n        mlp_units: A list of integers for the final MLP layer sizes.\\n        dropout: Dropout rate for the transformer block.\\n        mlp_dropout: Dropout rate for the final MLP layers.\\n\\n    Returns:\\n        An uncompiled tf.keras.Model for the Transformer.\\n    \\\"\\\"\\\"\\n    if not (isinstance(input_shape, tuple) and len(input_shape) == 2 and all(isinstance(dim, int) and dim > 0 for dim in input_shape)):\\n        raise ValueError(f\\\"Invalid input_shape: {input_shape}. Expected a tuple of two positive integers.\\\")\\n\\n    inputs = tf.keras.Input(shape=input_shape)\\n    x = PositionalEncoding(input_shape[0], input_shape[1])(inputs) # Hypothetical implementation\\n    x = layers.Dense(head_size)(x) # Project features to model dimension\\n\\n    for _ in range(num_transformer_blocks):\\n        x = TransformerEncoderBlock(head_size, num_heads, ff_dim, dropout)(x)\\n\\n    x = layers.GlobalAveragePooling1D(data_format=\\\"channels_last\\\")(x)\\n    for dim in mlp_units:\\n        x = layers.Dense(dim, activation=\\\"relu\\\")(x)\\n        x = layers.Dropout(mlp_dropout)(x)\\n    outputs = layers.Dense(1)(x)\\n\\n    return tf.keras.Model(inputs=inputs, outputs=outputs)\",\n  \"documentation\": \"This module provides functions to construct the neural network architectures for time-series prediction. It contains `build_lstm_model` and `build_transformer_model`. Each function takes the data input shape (determined by the preprocessing module) and returns a corresponding uncompiled TensorFlow Keras model. This module's scope is strictly limited to model definition; it does not handle data, training, compilation (i.e., setting optimizers or loss functions), or evaluation. The returned models are intended to be consumed by the `train_models` module, which will then compile and train them.\"\n}"