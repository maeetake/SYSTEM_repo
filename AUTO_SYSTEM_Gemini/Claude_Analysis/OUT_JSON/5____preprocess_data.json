"{\n  \"task_name\": \"preprocess_data\",\n  \"module_responsibility\": \"This module is exclusively responsible for cleaning, transforming, and structuring the raw time-series data. Its duties include handling missing values, scaling features, and creating sequential datasets (input sequences and corresponding targets) suitable for training and evaluating time-series models. It does not load data from files or build/train models.\",\n  \"model_role_and_purpose\": \"The purpose of this module is to convert the raw, loaded stock data into a clean, normalized, and structured format required by the downstream deep learning models (LSTM and Transformer). Proper preprocessing is critical for model stability, faster convergence, and accurate predictions. This module bridges the gap between raw data and the model training phase.\",\n  \"concrete_tasks\": [\n    {\n      \"task_id\": \"preprocess_data_1\",\n      \"function_name\": \"handle_missing_values\",\n      \"description\": \"Checks for and fills any missing values in the OHLC columns of the input DataFrame using the forward fill ('ffill') method.\",\n      \"input\": {\n        \"df\": \"pandas.DataFrame containing OHLC data.\"\n      },\n      \"output\": {\n        \"df_filled\": \"pandas.DataFrame with missing values handled.\"\n      }\n    },\n    {\n      \"task_id\": \"preprocess_data_2\",\n      \"function_name\": \"normalize_data\",\n      \"description\": \"Normalizes the OHLC feature columns to a range of [0, 1] using scikit-learn's MinMaxScaler. The fitted scaler object is returned for later use in inverse-transforming the predictions.\",\n      \"input\": {\n        \"df\": \"pandas.DataFrame with OHLC data.\"\n      },\n      \"output\": {\n        \"scaled_data\": \"numpy.ndarray of shape (n_samples, n_features) with values scaled between 0 and 1.\",\n        \"scaler\": \"sklearn.preprocessing.MinMaxScaler object fitted to the data.\"\n      }\n    },\n    {\n      \"task_id\": \"preprocess_data_3\",\n      \"function_name\": \"create_sequences\",\n      \"description\": \"Converts the time-series data into input sequences and corresponding target values. It uses a sliding window of 60 days of OHLC data (X) to predict the closing price of the 61st day (y).\",\n      \"input\": {\n        \"data\": \"numpy.ndarray of scaled OHLC data.\",\n        \"sequence_length\": \"integer, the number of past time steps to use as input features (e.g., 60).\"\n      },\n      \"output\": {\n        \"X\": \"numpy.ndarray of shape (n_samples, sequence_length, n_features) containing the input sequences.\",\n        \"y\": \"numpy.ndarray of shape (n_samples,) containing the target closing prices.\"\n      }\n    },\n    {\n        \"task_id\": \"preprocess_data_4\",\n        \"function_name\": \"split_data\",\n        \"description\": \"Splits the sequential data chronologically into training (80%), validation (10%), and test (10%) sets.\",\n        \"input\": {\n            \"X\": \"numpy.ndarray of input sequences.\",\n            \"y\": \"numpy.ndarray of target values.\"\n        },\n        \"output\": {\n            \"splits\": \"A dictionary containing six numpy arrays: 'X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'.\"\n        }\n    }\n  ],\n  \"dependencies\": [\n    \"load_data\"\n  ],\n  \"constraints\": {\n    \"library_versions_and_configurations\": \"Python 3.8+, pandas>=1.3.0, numpy>=1.20.0, scikit-learn>=1.0.0. The MinMaxScaler should use feature_range=(0, 1). The sequence length for creating windows must be fixed at 60.\",\n    \"error_handling\": \"The module must validate its input DataFrame. It should raise a `ValueError` if required columns ('Open', 'High', 'Low', 'Close') are missing or if the data contains non-numeric types. It should also raise a `ValueError` if the dataset has fewer than 61 rows, making it impossible to create a single sequence.\",\n    \"input_formats_and_data_types\": {\n      \"description\": \"A single pandas DataFrame provided by the `load_data` module. This DataFrame must contain at least the columns 'Open', 'High', 'Low', 'Close' with data types convertible to numeric (e.g., float64, int64).\",\n      \"schema\": {\n        \"type\": \"pandas.DataFrame\",\n        \"columns\": {\n          \"Open\": \"numeric\",\n          \"High\": \"numeric\",\n          \"Low\": \"numeric\",\n          \"Close\": \"numeric\"\n        }\n      }\n    },\n    \"output_formats_and_data_types\": {\n      \"description\": \"A dictionary containing the preprocessed and split data, along with the scaler object. The data arrays should be of type numpy.float32 for compatibility with deep learning frameworks.\",\n      \"schema\": {\n        \"type\": \"dict\",\n        \"keys\": {\n          \"X_train\": \"numpy.ndarray (shape: [num_train_samples, 60, 4], dtype: float32)\",\n          \"y_train\": \"numpy.ndarray (shape: [num_train_samples,], dtype: float32)\",\n          \"X_val\": \"numpy.ndarray (shape: [num_val_samples, 60, 4], dtype: float32)\",\n          \"y_val\": \"numpy.ndarray (shape: [num_val_samples,], dtype: float32)\",\n          \"X_test\": \"numpy.ndarray (shape: [num_test_samples, 60, 4], dtype: float32)\",\n          \"y_test\": \"numpy.ndarray (shape: [num_test_samples,], dtype: float32)\",\n          \"scaler\": \"sklearn.preprocessing.MinMaxScaler\"\n        }\n      }\n    },\n    \"specific_error_handling\": \"Any `ValueError` should be logged with a descriptive message indicating the cause (e.g., 'Input DataFrame is missing required columns', 'Not enough data to create sequences'). If missing values remain at the start of the dataframe after forward fill, those rows should be dropped, and a warning should be logged.\"\n  },\n  \"code_skeleton\": \"from typing import Dict, Tuple, List\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn.preprocessing import MinMaxScaler\\n\\nclass DataPreprocessor:\\n    def __init__(self, sequence_length: int = 60):\\n        \\\"\\\"\\\"\\n        Initializes the preprocessor with a fixed sequence length.\\n        \\\"\\\"\\\"\\n        self.sequence_length = sequence_length\\n        self.scaler = MinMaxScaler(feature_range=(0, 1))\\n        self.feature_cols = ['Open', 'High', 'Low', 'Close']\\n\\n    def _handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:\\n        \\\"\\\"\\\"Fills missing values using forward fill and drops any remaining NaNs.\\\"\\\"\\\"\\n        pass\\n\\n    def _normalize_data(self, df: pd.DataFrame) -> np.ndarray:\\n        \\\"\\\"\\\"Scales the feature columns using MinMaxScaler.\\\"\\\"\\\"\\n        pass\\n\\n    def _create_sequences(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\\n        \\\"\\\"\\\"Creates input sequences and target values from scaled data.\\\"\\\"\\\"\\n        pass\\n\\n    def _split_data(self, X: np.ndarray, y: np.ndarray) -> Dict[str, np.ndarray]:\\n        \\\"\\\"\\\"Splits data chronologically into train, validation, and test sets.\\\"\\\"\\\"\\n        pass\\n\\n    def process(self, df: pd.DataFrame) -> Dict[str, any]:\\n        \\\"\\\"\\\"\\n        Executes the full preprocessing pipeline.\\n\\n        Args:\\n            df (pd.DataFrame): The raw input DataFrame with OHLC data.\\n\\n        Returns:\\n            Dict[str, any]: A dictionary containing the split datasets (X_train, y_train, etc.)\\n                            and the fitted scaler object.\\n        \\\"\\\"\\\"\\n        # 1. Validate and select features\\n        # 2. Handle missing values\\n        # 3. Normalize data\\n        # 4. Create sequences\\n        # 5. Split data\\n        # 6. Return dictionary with all artifacts\\n        pass\\n\",\n  \"documentation\": \"This module, `preprocess_data`, serves as the data preparation engine for the time-series forecasting system. Its primary function is to take a raw DataFrame of stock prices and transform it into a state ready for model consumption. The process involves several sequential steps: validating input, handling missing data via forward fill, normalizing 'Open', 'High', 'Low', 'Close' features to a [0, 1] range, and structuring the data into sequences of 60 time steps to predict the 61st day's closing price. The module outputs a dictionary containing chronologically split training (80%), validation (10%), and test (10%) sets, along with the scaler used for normalization. This module will not perform data loading or model training; it strictly focuses on data transformation, ensuring a clean and consistent input for the `train_model` module.\"\n}"